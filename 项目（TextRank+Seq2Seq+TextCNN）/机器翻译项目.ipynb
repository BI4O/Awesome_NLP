{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Seq2Seq模型实现.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMHG8fL1J48i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "from collections import Counter #计数器\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx4dAeBzJ48u",
        "colab_type": "text"
      },
      "source": [
        "### load_data函数：\n",
        "\n",
        "- 输入的是文件\n",
        "- 输出的是一个训练集和验证机列表\n",
        "  - 列表中每个元素是一个句子列表，列表由字组成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOVJ0ggpJ49k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "dcedeaf3-e0d4-4d68-8461-50e90e69f3e7"
      },
      "source": [
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8TOjcPpJ49s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "2af962f4-5599-48d3-e87d-818c40b11622"
      },
      "source": [
        "from snownlp import SnowNLP\n",
        "import jieba\n",
        "def load_data(in_file):\n",
        "    cn = []\n",
        "    en = []\n",
        "    num_examples = 0\n",
        "    with open(in_file,'r',encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip().split('\\t')\n",
        "            en.append(['BOS']+nltk.word_tokenize(line[0].lower())+['EOS'])\n",
        "            # 转化为简体中文\n",
        "            cn_simple = SnowNLP(line[1]).han\n",
        "            # 使用切词分割中文\n",
        "            cn_words = jieba.cut(cn_simple)\n",
        "#             cn_words = SnowNLP(cn_simple).words\n",
        "            cn.append(['BOS']+[c for c in cn_words]+['EOS'])\n",
        "    return en, cn\n",
        "\n",
        "train_file = \"/content/nmt/nmt/en-cn/train.txt\"\n",
        "dev_file = \"/content/nmt/nmt/en-cn/dev.txt\"\n",
        "train_en, train_cn = load_data(train_file)\n",
        "dev_en, dev_cn = load_data(dev_file)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 1.132 seconds.\n",
            "Prefix dict has been built succesfully.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19J7sUonJ49x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "bd7037ee-75a6-4a26-df5a-520ce43358fb"
      },
      "source": [
        "print(train_en[:5])\n",
        "print(train_cn[:5])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['BOS', 'anyone', 'can', 'do', 'that', '.', 'EOS'], ['BOS', 'how', 'about', 'another', 'piece', 'of', 'cake', '?', 'EOS'], ['BOS', 'she', 'married', 'him', '.', 'EOS'], ['BOS', 'i', 'do', \"n't\", 'like', 'learning', 'irregular', 'verbs', '.', 'EOS'], ['BOS', 'it', \"'s\", 'a', 'whole', 'new', 'ball', 'game', 'for', 'me', '.', 'EOS']]\n",
            "[['BOS', '任何人', '都', '可以', '做到', '。', 'EOS'], ['BOS', '要', '不要', '再', '来', '一块', '蛋糕', '？', 'EOS'], ['BOS', '她', '嫁给', '了', '他', '。', 'EOS'], ['BOS', '我', '不', '喜欢', '学习', '不规则', '动词', '。', 'EOS'], ['BOS', '这', '对', '我', '来说', '是', '个', '全新', '的', '球类', '游戏', '。', 'EOS']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoeViCYVJ4-C",
        "colab_type": "text"
      },
      "source": [
        "## build_dict函数构建单词表\n",
        "\n",
        "- 输入的是上一步得到的句子列表\n",
        "\n",
        "- 输出的是word2ix字典{\"word1\":1, \"word2\":2}, 以及字典的长度\n",
        "\n",
        "- 最后还构建一个ix2word字典"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wB46cn2J4-E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bc9d3e0d-0b7d-43de-99a8-c2338632afe7"
      },
      "source": [
        "\"\"\"构建单词表, 重点\"\"\"\n",
        "UNK_IDX = 0\n",
        "PAD_IDX = 1\n",
        "def build_dict(sentences,max_words=100000):\n",
        "    word_count = Counter()\n",
        "    for sentence in sentences:\n",
        "        for word in sentence:\n",
        "            word_count[word] += 1\n",
        "    ls = word_count.most_common(max_words)\n",
        "#     print(len(ls)) # train_en:5491\n",
        "    total_words = len(ls)+2 # 因为有unk和pad\n",
        "    \n",
        "    # 构造字典\n",
        "    word_dict = {w[0]: index+2 for index, w in enumerate(ls)}\n",
        "    word_dict[\"UNK\"] = UNK_IDX\n",
        "    word_dict[\"PAD\"] = PAD_IDX\n",
        "    \n",
        "    # 返回做好的word2ix字典， 以及这个字典的容量\n",
        "    return word_dict, total_words\n",
        "\n",
        "# 调用函数进行建表\n",
        "en_dict, en_total_words = build_dict(train_en)\n",
        "cn_dict, cn_total_words = build_dict(train_cn)\n",
        "\n",
        "# 构造索引到字的反向字典\n",
        "inv_en_dict = {v: k for k,v in en_dict.items()}\n",
        "inv_cn_dict = {v: k for k,v in cn_dict.items()}\n",
        "\n",
        "print(f\"中文字典的长度：{cn_total_words}\")\n",
        "print(f\"英语字典的长度：{en_total_words}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "中文字典的长度：8883\n",
            "英语字典的长度：5493\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuqnVl-qJ4-S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"把文字全部转变为数字\"\"\"\n",
        "\n",
        "def encode(en_sen, cn_sen, en_dict, cn_dict, sort_by_len=True):\n",
        "    # en_sen = [['BOS', 'anyone', 'can', 'do', 'that', '.', 'EOS'],..\n",
        "    length = len(en_sen)\n",
        "    \n",
        "    # 把每个字都进行数字化，依然保留列表全套列表的结构，表示字在句中，句在文中\n",
        "    out_en_sen = [[en_dict.get(w, 0) for w in sent] for sent in en_sen]\n",
        "    out_cn_sen = [[cn_dict.get(w, 0) for w in sent] for sent in cn_sen]\n",
        "    \n",
        "    # 定义一个根据句子长度进行排序的方法\n",
        "    def len_argsort(seq):\n",
        "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
        "    \n",
        "    if sort_by_len == True:\n",
        "        sorted_index = len_argsort(out_en_sen)\n",
        "        \n",
        "        out_en_sen = [out_en_sen[i] for i in sorted_index]\n",
        "        out_cn_sen = [out_cn_sen[i] for i in sorted_index]\n",
        "        \n",
        "    return out_en_sen, out_cn_sen\n",
        "\n",
        "train_en, train_cn = encode(train_en, train_cn, en_dict, cn_dict)\n",
        "dev_en, dev_cn = encode(dev_en, dev_cn, en_dict, cn_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFpit0KIJ4-W",
        "colab_type": "text"
      },
      "source": [
        "### 看看处理好的句子"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI4DWbvIJ4-c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "87dfc807-7517-4210-91d0-a4ead8c8f69f"
      },
      "source": [
        "k=123\n",
        "print(\" \".join([inv_cn_dict[i] for i in train_cn[k]])) #通过inv字典获取单词\n",
        "print(\" \".join([inv_en_dict[i] for i in train_en[k]])) "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BOS 抓住 他 。 EOS\n",
            "BOS grab him . EOS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb5S8NZ2J4-h",
        "colab_type": "text"
      },
      "source": [
        "### 把全部句子处理成为batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx2oiEpzJ4-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_minibatch(n,minibatch_size, shuffle=True):\n",
        "    idx_list = np.arange(0, n, minibatch_size)\n",
        "    if shuffle:\n",
        "        np.random.shuffle(idx_list)# 打乱数据\n",
        "    minibatches = []\n",
        "    for idx in idx_list:\n",
        "        minibatches.append(np.arange(idx, min(idx + minibatch_size, n)))\n",
        "    return minibatches\n",
        "        \n",
        "def prepare_data(seqs):\n",
        "    # 统计每个batch里面的语句的长度\n",
        "    lengths = [len(seq) for seq in seqs]\n",
        "    # 一个batch有多少个语句\n",
        "    n_samples = len(seqs)\n",
        "    # 取出最长的语句，后面用这个来左padding基准\n",
        "    max_len = np.max(lengths)\n",
        "    \n",
        "    # 先初始化全部为零的矩阵，后面进行赋值\n",
        "    x = np.zeros((n_samples, max_len)).astype('int32')\n",
        "    x_lengths = np.array(lengths).astype('int32')\n",
        "    \n",
        "    # 取出一个bantch的每条语句和对应的索引\n",
        "    for idx,seq in enumerate(seqs):\n",
        "        # 把每条语句按行赋值给x，x会有一些零值没有被赋值\n",
        "        x[idx, :lengths[idx]] = seq\n",
        "        \n",
        "    return x, x_lengths\n",
        "\n",
        "def gen_examples(en_sentences, cn_sentences, batch_size):\n",
        "    minibatches = get_minibatch(len(en_sentences), batch_size)\n",
        "    all_ex = []\n",
        "    for minibatch in minibatches:\n",
        "        mb_en_sentences = [en_sentences[t] for t in minibatch]\n",
        "        mb_cn_sentences = [cn_sentences[t] for t in minibatch]\n",
        "        mb_x,mb_x_len = prepare_data(mb_en_sentences)\n",
        "        mb_y,mb_y_len = prepare_data(mb_cn_sentences)\n",
        "        \n",
        "        all_ex.append((mb_x, mb_x_len, mb_y, mb_y_len))\n",
        "        \n",
        "    return all_ex\n",
        "\n",
        "batch_size = 64\n",
        "train_data = gen_examples(train_en, train_cn, batch_size)\n",
        "random.shuffle(train_data)\n",
        "dev_data = gen_examples(dev_en, dev_cn, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "KT1gUVShJ4-n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c9062a76-b73c-4542-bd9f-013253af3cd7"
      },
      "source": [
        "train_data[0]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[   2,   31,   14,    8,   53,   11,    3],\n",
              "        [   2,    6,  524,   10, 1614,    4,    3],\n",
              "        [   2,   71,  229,   34,   61,  126,    3],\n",
              "        [   2,   32,  100,   10,  452,    4,    3],\n",
              "        [   2,   18,  563,  288,  856,    4,    3],\n",
              "        [   2,   44,   30,   71,  773,    4,    3],\n",
              "        [   2,   14,    8,  260, 2600,   11,    3],\n",
              "        [   2, 3613,   30,  120, 1087,    4,    3],\n",
              "        [   2,   87,   30,   32,  344,   11,    3],\n",
              "        [   2,    5,  273,   25,  469,    4,    3],\n",
              "        [   2,    5,   63,    9, 2104,    4,    3],\n",
              "        [   2,    5,  113,    9,  249,    4,    3],\n",
              "        [   2,   30,   44,  116,  405,   11,    3],\n",
              "        [   2,   18,   85,   13,  282,    4,    3],\n",
              "        [   2,   67,    5,  110,    8,   11,    3],\n",
              "        [   2,    6, 2488, 2613, 1105,    4,    3],\n",
              "        [   2, 1518, 1676,   34,  106,    4,    3],\n",
              "        [   2,    5,   56, 2614,    8,    4,    3],\n",
              "        [   2,    5,   42,  130,  726,    4,    3],\n",
              "        [   2,    5,   76,   64, 3626,    4,    3],\n",
              "        [   2,   87,   10,   32,  189,   11,    3],\n",
              "        [   2,  642,   77, 2112, 1123,    4,    3],\n",
              "        [   2,   18,   27, 2046,  718,    4,    3],\n",
              "        [   2,   51,  247,   15, 1486,    4,    3],\n",
              "        [   2,   21,  100,  860,  145,    4,    3],\n",
              "        [   2,   12,   20,  336, 3633,    4,    3],\n",
              "        [   2,    5,  136,  110,   40,    4,    3],\n",
              "        [   2,   91,   23, 2115,   25,    4,    3],\n",
              "        [   2,    5,   93,   16,  485,    4,    3],\n",
              "        [   2,  230, 1520, 1323, 3637,    4,    3],\n",
              "        [   2,   58,  270,    6,  210,    4,    3],\n",
              "        [   2,   87,   70,   16,  661,   11,    3],\n",
              "        [   2,   14,    8,  247,   59,   11,    3],\n",
              "        [   2,   12,   20,  120, 2626,    4,    3],\n",
              "        [   2,   12,  320,   65,  393,    4,    3],\n",
              "        [   2,    8,   90, 2569,  450,    4,    3],\n",
              "        [   2,   10,   13,   16,  662,   11,    3],\n",
              "        [   2,   25, 1973, 3651,    8,    4,    3],\n",
              "        [   2,  132,  185,    7,  252,    4,    3],\n",
              "        [   2,   91,   20,   73,  625,    4,    3],\n",
              "        [   2,   51,  730,  125, 2630,    4,    3],\n",
              "        [   2,   19,   48,  714,  416,    4,    3],\n",
              "        [   2,   87,   30,   32,  344,   11,    3],\n",
              "        [   2,   18,   10,  105,  361,    4,    3],\n",
              "        [   2,   12, 1732,   34,  101,    4,    3],\n",
              "        [   2,   31,  740,  345,   44,   11,    3],\n",
              "        [   2,   87,   20,   32,  189,   11,    3],\n",
              "        [   2,   29,  180, 1523,  619,    4,    3],\n",
              "        [   2,    6,  189,  123,  204,    4,    3],\n",
              "        [   2,   18,   10,  817,   95,    4,    3],\n",
              "        [   2,   16,   20,    9,  824,    4,    3],\n",
              "        [   2,  304, 2128,   23, 1120,    4,    3],\n",
              "        [   2,    8,   79,   83,   63,    4,    3],\n",
              "        [   2,   87,   30,   32,  344,   11,    3],\n",
              "        [   2,    5,   56,  183,  106,    4,    3],\n",
              "        [   2,   25,   10,   37,  377,    4,    3],\n",
              "        [   2,    5,  475,  117,  103,    4,    3],\n",
              "        [   2,    5,   84,   57,  291,    4,    3],\n",
              "        [   2,   28,   27,   13,  311,    4,    3],\n",
              "        [   2,  341, 2474,   30,  209,    4,    3],\n",
              "        [   2,    6,  182,   48,  307,    4,    3],\n",
              "        [   2,  233,  248,   10,  343,    4,    3],\n",
              "        [   2,   18,  265,    7,  252,    4,    3],\n",
              "        [   2,   31,    9,   66,  379,  126,    3]], dtype=int32),\n",
              " array([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
              "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
              "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
              "       dtype=int32),\n",
              " array([[   2,    8,   35,   22,   10,    3,    0,    0,    0],\n",
              "        [   2,  423,  105, 1525,    4,    3,    0,    0,    0],\n",
              "        [   2,   22,   76,   30,   27,   93,    3,    0,    0],\n",
              "        [   2,    8,  259,   19,  247,    4,    3,    0,    0],\n",
              "        [   2,   16, 5050, 5051,    4,    3,    0,    0,    0],\n",
              "        [   2,   27,   76,    4,    3,    0,    0,    0,    0],\n",
              "        [   2,    8,  126, 1968,   14,   10,    3,    0,    0],\n",
              "        [   2, 5068,  136,   40,   37, 1520,    4,    3,    0],\n",
              "        [   2,   66,    6,   92,   11,   85,   10,    3,    0],\n",
              "        [   2,    5,  278,   24, 5072,    4,    3,    0,    0],\n",
              "        [   2,    5,   20,   25, 5073,    4,    3,    0,    0],\n",
              "        [   2,    5,   81,   50,  108,    4,    3,    0,    0],\n",
              "        [   2,   20,   22,   76,   14,   10,    3,    0,    0],\n",
              "        [   2,   16,   90,  156,    4,    3,    0,    0,    0],\n",
              "        [   2,    5,  134,  138,    8,   14,   10,    3,    0],\n",
              "        [   2, 5091, 5092, 5093,  204,  397, 1110,    4,    3],\n",
              "        [   2, 1041,   73, 2262,  107,    4,    3,    0,    0],\n",
              "        [   2,    5, 5098,    8,    4,    3,    0,    0,    0],\n",
              "        [   2,    5, 5102,    4,    3,    0,    0,    0,    0],\n",
              "        [   2,    5,   13,  365, 5105,    4,    3,    0,    0],\n",
              "        [   2,    8,    6,  137,   11,   85,   10,    3,    0],\n",
              "        [   2,  505,   65,   81, 5107,    4,    3,    0,    0],\n",
              "        [   2,   16,   11, 1679,   59, 5111,    7,    4,    3],\n",
              "        [   2,   28,   86,   78, 1621,    6,  264,    4,    3],\n",
              "        [   2,    5,   89, 3285,    7,    4,    3,    0,    0],\n",
              "        [   2,    9,   11, 1977,    4,    3,    0,    0,    0],\n",
              "        [   2,    5,  100,  170,   12,    4,    3,    0,    0],\n",
              "        [   2,   40,    5,   42,  866,    4,    3,    0,    0],\n",
              "        [   2,    5,   84,   26,    6,    4,    3,    0,    0],\n",
              "        [   2, 3289, 1142, 5128, 5129,    4,    3,    0,    0],\n",
              "        [   2,   44,  986,  382,    4,    3,    0,    0,    0],\n",
              "        [   2,   85, 1496,   32,    3,    0,    0,    0,    0],\n",
              "        [   2,    8,   99,   56,   14,   32,    3,    0,    0],\n",
              "        [   2,    9,  136,   11,  449,    4,    3,    0,    0],\n",
              "        [   2,    9,   25,   37,  264,    4,    3,    0,    0],\n",
              "        [   2,    8, 2501,    7,    4,    3,    0,    0,    0],\n",
              "        [   2,   57,   52,  855,    6,   14,   10,    3,    0],\n",
              "        [   2,   23,  406,    8,   20, 1572,  954,    4,    3],\n",
              "        [   2,  506,  191,  160,  174,    4,    3,    0,    0],\n",
              "        [   2,   40,   15,  107,  120,    4,    3,    0,    0],\n",
              "        [   2,   28,  509,    7,  205,  373,    4,    3,    0],\n",
              "        [   2,   12,   20,  914,    6,  408,    4,    3,    0],\n",
              "        [   2,    8,    6,   92,   11,   85,   32,    3,    0],\n",
              "        [   2,   16,   77, 5170,    4,    3,    0,    0,    0],\n",
              "        [   2,    9,  782,  772,    7,    4,    3,    0,    0],\n",
              "        [   2,  131, 2369,  127,    7,   22,   75,   10,    3],\n",
              "        [   2,    8,    6,  137,   11,   85,   10,    3,    0],\n",
              "        [   2,   15, 2506,  242,    4,    3,    0,    0,    0],\n",
              "        [   2,   23,  203,  137,  347,  142,    7,    4,    3],\n",
              "        [   2,   16,   68,   13, 1120,    6,    4,    3,    0],\n",
              "        [   2,   57,   13,   50,  867,    4,    3,    0,    0],\n",
              "        [   2,  163, 3110,    5,   49,  880,    4,    3,    0],\n",
              "        [   2,  145,   59, 5196,    7,    4,    3,    0,    0],\n",
              "        [   2,    8,    6,   92,   11,   85,   10,    3,    0],\n",
              "        [   2,   72,  768,    4,    3,    0,    0,    0,    0],\n",
              "        [   2,   23,   18,  299,    4,    3,    0,    0,    0],\n",
              "        [   2,    5,  144,  974,    4,    3,    0,    0,    0],\n",
              "        [   2,    5,  325,  498,    4,    3,    0,    0,    0],\n",
              "        [   2,   64,  480,    4,    3,    0,    0,    0,    0],\n",
              "        [   2,  403, 3029,   19,  417,    4,    3,    0,    0],\n",
              "        [   2, 1170,  159,   69,   49,    7,    4,    3,    0],\n",
              "        [   2, 1064,   19, 1045,    4,    3,    0,    0,    0],\n",
              "        [   2,   16,   36,  174,    4,    3,    0,    0,    0],\n",
              "        [   2, 3335,    6,  724,  364,   93,    3,    0,    0]],\n",
              "       dtype=int32),\n",
              " array([6, 6, 7, 7, 6, 5, 7, 8, 8, 7, 7, 7, 7, 6, 8, 9, 7, 6, 5, 7, 8, 7,\n",
              "        9, 9, 7, 6, 7, 7, 7, 7, 6, 5, 7, 7, 7, 6, 8, 9, 7, 7, 8, 8, 8, 6,\n",
              "        7, 9, 8, 6, 9, 8, 7, 8, 7, 8, 5, 6, 6, 6, 5, 7, 8, 6, 6, 7],\n",
              "       dtype=int32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbJ7-iePJ5BN",
        "colab_type": "text"
      },
      "source": [
        "## 数据处理完毕，开始建模"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y3xcJW8J5BU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"luong Attention版本\"\"\"\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,vocab_size, embed_size, \n",
        "                 enc_hidden_size, dec_hidden_size,\n",
        "                 dropout=0.2):\n",
        "        super().__init__()\n",
        "        # 嵌入层，把二维数据变成三维数据\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        # rnn层，把embed_size跟enc_hidden_size拼接进行运算\n",
        "        self.rnn = nn.GRU(embed_size, enc_hidden_size,\n",
        "                          batch_first=True, bidirectional=True)\n",
        "        # 常规的dropout层\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # 全连接层，因为最后只拿hidden来进行解码\n",
        "        self.fc = nn.Linear(enc_hidden_size * 2, dec_hidden_size)\n",
        "        \n",
        "    def forward(self, x, lengths):\n",
        "        sorted_len, sorted_idx = lengths.sort(0, descending=True)\n",
        "        x_sorted = x[sorted_idx.long()]\n",
        "        # 第一步：进行词嵌入\n",
        "        embedded = self.dropout(self.embed(x_sorted))\n",
        "        \n",
        "        # 第二步：对embeded数据进行padd\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
        "                            embedded,\n",
        "                            sorted_len.long().cpu().data.numpy(),\n",
        "                            batch_first=True)\n",
        "        \n",
        "        # 第三步：把已经补长到相同长度的batch喂给rnn(GRU)\n",
        "        # 因为是双向的，所以得到的应该是两份的hidden\n",
        "        # hid.shape = [1, 64, 100]\n",
        "        packed_out, hid = self.rnn(packed_embedded)\n",
        "        \n",
        "        # 第四步：把补长过的输出还原成没有补长前\n",
        "        # out.shape = [64, 10, 100]\n",
        "        out, _ = nn.utils.rnn.pad_packed_sequence(\n",
        "                            packed_out,\n",
        "                            batch_first=True)\n",
        "        \n",
        "        # 第五步：\n",
        "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
        "        \n",
        "        # 第六步：\n",
        "        #out.shape = torch.Size([64, 10, 100])\n",
        "        #hid.shape = torch.Size([1, 64, 100])\n",
        "        out = out[original_idx.long()].contiguous()\n",
        "        hid = hid[:, original_idx.long()].contiguous()\n",
        "        \n",
        "        # 第七步：把最后的两个hidden拼接起来\n",
        "        hid = torch.cat([hid[-2], hid[-1]], dim=1)\n",
        "        hid = torch.tanh(self.fc(hid)).unsqueeze(0)\n",
        "        \n",
        "        return out, hid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0QbLHfBJ5Ba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hidden_size, dec_hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "        \n",
        "        self.enc_hidden_size = enc_hidden_size\n",
        "        self.dec_hidden_size = dec_hidden_size\n",
        "        \n",
        "        self.linear_in = nn.Linear(enc_hidden_size*2, dec_hidden_size)\n",
        "        self.linear_out = nn.Linear(enc_hidden_size*2+dec_hidden_size,\n",
        "                                   dec_hidden_size)\n",
        "        \n",
        "    def forward(self, output, context, mask):\n",
        "        # output: [batch_size, output_len, dec_hidden_size]\n",
        "        # context; [batch_size, context_len, 2*enc_hidden_size]\n",
        "        \n",
        "        batch_size = output.size(0)\n",
        "        output_len = output.size(1)\n",
        "        input_len = context.size(1)\n",
        "        \n",
        "        # 第一步：把context vect进行线性变换到跟input一样的规格\n",
        "        # batch_size, context_len, dec_hidden_size\n",
        "        context_in = self.linear_in(context.view(batch_size*input_len, -1)).view(\n",
        "            batch_size, input_len, -1)\n",
        "        \n",
        "        # 第二步：进行张量乘法\n",
        "        # transpose: batch_size, dec_hidden_size, context_len\n",
        "        # output: batch_size, output_len, dec_hidden_size\n",
        "        attn = torch.bmm(output, context_in.transpose(1,2))\n",
        "        # attn: batch_size, output_len, context_len\n",
        "        \n",
        "        # 第三步：\n",
        "        attn.data.masked_fill(mask, -1e6)\n",
        "        \n",
        "        # 第四步：attnetion weight进行softmax得到attention vect\n",
        "        # attn: batch_size, output_len, context_len\n",
        "        attn = F.softmax(attn, dim=2)\n",
        "        \n",
        "        # 第五步：计算context vect\n",
        "        # context: batch_size, output_len, enc_hidden_size\n",
        "        context = torch.bmm(attn, context)\n",
        "        \n",
        "        # 第六步：把context向量与output向量合并一下\n",
        "        # batch_size, output_len, hidden_size*2\n",
        "        output = torch.cat((context, output), dim=2)\n",
        "        output = output.view(batch_size*output_len, -1)\n",
        "        output = torch.tanh(self.linear_out(output))\n",
        "        output = output.view(batch_size, output_len, -1)\n",
        "        return output, attn\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut-K_caBJ5Bf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.attention = Attention(enc_hidden_size, dec_hidden_size)\n",
        "        self.rnn = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(dec_hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def create_mask(self, x_len, y_len):\n",
        "        # a mask of shape x_len * y_len\n",
        "        device = x_len.device\n",
        "        max_x_len = x_len.max()\n",
        "        max_y_len = y_len.max()\n",
        "        x_mask = torch.arange(max_x_len, device=x_len.device)[None, :] < x_len[:, None]\n",
        "        y_mask = torch.arange(max_y_len, device=x_len.device)[None, :] < y_len[:, None]\n",
        "        mask = (1 - x_mask[:, :, None] * y_mask[:, None, :]).byte()\n",
        "        return mask\n",
        "    \n",
        "    def forward(self, ctx, ctx_lengths, y, y_lengths, hid):\n",
        "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)\n",
        "        y_sorted = y[sorted_idx.long()]\n",
        "        hid = hid[:, sorted_idx.long()]\n",
        "        \n",
        "        y_sorted = self.dropout(self.embed(y_sorted)) # batch_size, output_length, embed_size\n",
        "\n",
        "        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
        "        out, hid = self.rnn(packed_seq, hid)\n",
        "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
        "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
        "        output_seq = unpacked[original_idx.long()].contiguous()\n",
        "        hid = hid[:, original_idx.long()].contiguous()\n",
        "\n",
        "        mask = self.create_mask(y_lengths, ctx_lengths)\n",
        "\n",
        "        output, attn = self.attention(output_seq, ctx, mask)\n",
        "        output = F.log_softmax(self.out(output), -1)\n",
        "        \n",
        "        return output, hid, attn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tIxcgRgJ5Bt",
        "colab_type": "text"
      },
      "source": [
        "### 最后构建seq2seq模型把encder, attention, decoder串在一起"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_c9vUatJ5Bu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"完整的模型\"\"\"\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        \n",
        "    def forward(self, x, x_lengths, y, y_lengths):\n",
        "        encoder_out, hid = self.encoder(x, x_lengths)\n",
        "        output, hid, attn = self.decoder(ctx=encoder_out, \n",
        "                    ctx_lengths=x_lengths,\n",
        "                    y=y,\n",
        "                    y_lengths=y_lengths,\n",
        "                    hid=hid)\n",
        "        return output, attn\n",
        "    \n",
        "    def translate(self, x, x_lengths, y, max_length=100):\n",
        "        encoder_out, hid = self.encoder(x, x_lengths)\n",
        "        preds = []\n",
        "        batch_size = x.shape[0]\n",
        "        attns = []\n",
        "        for i in range(max_length):\n",
        "            output, hid, attn = self.decoder(ctx=encoder_out, \n",
        "                    ctx_lengths=x_lengths,\n",
        "                    y=y,\n",
        "                    y_lengths=torch.ones(batch_size).long().to(y.device),\n",
        "                    hid=hid)\n",
        "            y = output.max(2)[1].view(batch_size, 1)\n",
        "            preds.append(y)\n",
        "            attns.append(attn)\n",
        "        return torch.cat(preds, 1), torch.cat(attns, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc4qqlZsJ5B4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# masked cross entropy loss\n",
        "class LanguageModelCriterion(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LanguageModelCriterion, self).__init__()\n",
        "\n",
        "    def forward(self, input, target, mask):\n",
        "        #target=tensor([[5,108,8,4,3,0,0,0,0,0,0,0],....\n",
        "        #  mask=tensor([[1,1 ,1,1,1,0,0,0,0,0,0,0],.....\n",
        "        #print(input.shape,target.shape,mask.shape)\n",
        "        #torch.Size([64, 12, 3195]) torch.Size([64, 12]) torch.Size([64, 12])\n",
        "        \n",
        "        # input: (batch_size * seq_len) * vocab_size\n",
        "        input = input.contiguous().view(-1, input.size(2))\n",
        "        \n",
        "        # target: batch_size * 1=768*1\n",
        "        target = target.contiguous().view(-1, 1)\n",
        "        mask = mask.contiguous().view(-1, 1)\n",
        "        #print(-input.gather(1, target))\n",
        "        output = -input.gather(1, target) * mask\n",
        "#这里算得就是交叉熵损失，前面已经算了F.log_softmax\n",
        "#.gather的作用https://blog.csdn.net/edogawachia/article/details/80515038\n",
        "#output.shape=torch.Size([768, 1])\n",
        "#mask作用是把padding为0的地方重置为零，因为input.gather时，为0的地方不是零了\n",
        "        \n",
        "        output = torch.sum(output) / torch.sum(mask)\n",
        "        #均值损失\n",
        "\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0oLGHiSJ5CF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"训练配置\"\"\"\n",
        "dropout = 0.2\n",
        "embed_size = hidden_size = 100\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "encoder = Encoder(vocab_size=en_total_words,\n",
        "                       embed_size=embed_size,\n",
        "                      enc_hidden_size=hidden_size,\n",
        "                       dec_hidden_size=hidden_size,\n",
        "                      dropout=dropout)\n",
        "decoder = Decoder(vocab_size=cn_total_words,\n",
        "                      embed_size=embed_size,\n",
        "                      enc_hidden_size=hidden_size,\n",
        "                       dec_hidden_size=hidden_size,\n",
        "                      dropout=dropout)\n",
        "# 构建模型\n",
        "model = Seq2Seq(encoder, decoder)\n",
        "model = model.to(device)\n",
        "# 定义损失函数\n",
        "loss_fn = LanguageModelCriterion().to(device)\n",
        "# 定义优化器\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fgdN_zqJ5CL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"定义训练函数\"\"\"\n",
        "def train(model, data, num_epochs=2):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_num_words = total_loss = 0.\n",
        "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
        "            #（英文batch，英文长度，中文batch，中文长度）\n",
        "            \n",
        "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
        "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
        "            \n",
        "            #前n-1个单词作为输入，后n-1个单词作为输出，因为输入的前一个单词要预测后一个单词\n",
        "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
        "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
        "            #\n",
        "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
        "            #输入输出的长度都减一。\n",
        "            \n",
        "            mb_y_len[mb_y_len<=0] = 1\n",
        "            \n",
        "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
        "            #返回的是类PlainSeq2Seq里forward函数的两个返回值\n",
        "            \n",
        "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
        "#mb_out_mask=tensor([[1, 1, 1,  ..., 0, 0, 0],[1, 1, 1,  ..., 0, 0, 0],\n",
        "#mb_out_mask.shape= (64*19),这句代码咱不懂，这个mask就是padding的位置设置为0，其他设置为1\n",
        "#mb_out_mask就是LanguageModelCriterion的传入参数mask。\n",
        "\n",
        "            mb_out_mask = mb_out_mask.float()\n",
        "            \n",
        "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
        "            \n",
        "            num_words = torch.sum(mb_y_len).item()\n",
        "            #一个batch里多少个单词\n",
        "            \n",
        "            total_loss += loss.item() * num_words\n",
        "            #总损失，loss计算的是均值损失，每个单词都是都有损失，所以乘以单词数\n",
        "            \n",
        "            total_num_words += num_words\n",
        "            #总单词数\n",
        "            \n",
        "            # 更新模型\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.)\n",
        "            #为了防止梯度过大，设置梯度的阈值\n",
        "            \n",
        "            optimizer.step()\n",
        "            \n",
        "            if it % 100 == 0:\n",
        "                print(\"Epoch\", epoch, \"iteration\", it, \"loss\", loss.item())\n",
        "\n",
        "                \n",
        "        print(\"Epoch\", epoch, \"Training loss\", total_loss/total_num_words)\n",
        "        if epoch % 5 == 0:\n",
        "            evaluate(model, dev_data) #评估模型\n",
        "\n",
        "\"\"\"定义评价函数\"\"\"\n",
        "def evaluate(model, data):\n",
        "    model.eval()\n",
        "    total_num_words = total_loss = 0.\n",
        "    with torch.no_grad():#不需要更新模型，不需要梯度\n",
        "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
        "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
        "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
        "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
        "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
        "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
        "            mb_y_len[mb_y_len<=0] = 1\n",
        "\n",
        "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
        "\n",
        "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
        "            mb_out_mask = mb_out_mask.float()\n",
        "\n",
        "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
        "\n",
        "            num_words = torch.sum(mb_y_len).item()\n",
        "            total_loss += loss.item() * num_words\n",
        "            total_num_words += num_words\n",
        "    print(\"Evaluation loss\", total_loss/total_num_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMVjIqHrJ5CQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9e9ed903-2daa-4f26-bb6a-c2f6aa54923a"
      },
      "source": [
        "\"\"\"开始训练\"\"\"\n",
        "train(model, train_data, num_epochs=30)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 iteration 0 loss 9.110408782958984\n",
            "Epoch 0 iteration 100 loss 5.517004489898682\n",
            "Epoch 0 iteration 200 loss 5.55142068862915\n",
            "Epoch 0 Training loss 5.638189344529993\n",
            "Evaluation loss 4.96490495137933\n",
            "Epoch 1 iteration 0 loss 4.367475986480713\n",
            "Epoch 1 iteration 100 loss 4.888298511505127\n",
            "Epoch 1 iteration 200 loss 5.1074700355529785\n",
            "Epoch 1 Training loss 4.715048927505989\n",
            "Epoch 2 iteration 0 loss 3.866223096847534\n",
            "Epoch 2 iteration 100 loss 4.579425811767578\n",
            "Epoch 2 iteration 200 loss 4.786312580108643\n",
            "Epoch 2 Training loss 4.364312815570024\n",
            "Epoch 3 iteration 0 loss 3.5176448822021484\n",
            "Epoch 3 iteration 100 loss 4.323484420776367\n",
            "Epoch 3 iteration 200 loss 4.583402633666992\n",
            "Epoch 3 Training loss 4.102632323583446\n",
            "Epoch 4 iteration 0 loss 3.245243787765503\n",
            "Epoch 4 iteration 100 loss 4.094987869262695\n",
            "Epoch 4 iteration 200 loss 4.393760681152344\n",
            "Epoch 4 Training loss 3.881842375799318\n",
            "Epoch 5 iteration 0 loss 3.0458548069000244\n",
            "Epoch 5 iteration 100 loss 3.877596616744995\n",
            "Epoch 5 iteration 200 loss 4.228583335876465\n",
            "Epoch 5 Training loss 3.687873029178315\n",
            "Evaluation loss 4.0356513084111825\n",
            "Epoch 6 iteration 0 loss 2.842890501022339\n",
            "Epoch 6 iteration 100 loss 3.6883342266082764\n",
            "Epoch 6 iteration 200 loss 4.069833278656006\n",
            "Epoch 6 Training loss 3.513823193067291\n",
            "Epoch 7 iteration 0 loss 2.6446683406829834\n",
            "Epoch 7 iteration 100 loss 3.497162342071533\n",
            "Epoch 7 iteration 200 loss 3.943114757537842\n",
            "Epoch 7 Training loss 3.3570012807476166\n",
            "Epoch 8 iteration 0 loss 2.5158259868621826\n",
            "Epoch 8 iteration 100 loss 3.335132598876953\n",
            "Epoch 8 iteration 200 loss 3.8228442668914795\n",
            "Epoch 8 Training loss 3.2157451480683865\n",
            "Epoch 9 iteration 0 loss 2.3776047229766846\n",
            "Epoch 9 iteration 100 loss 3.2250046730041504\n",
            "Epoch 9 iteration 200 loss 3.7386960983276367\n",
            "Epoch 9 Training loss 3.082322709013677\n",
            "Epoch 10 iteration 0 loss 2.2661526203155518\n",
            "Epoch 10 iteration 100 loss 3.045462131500244\n",
            "Epoch 10 iteration 200 loss 3.6083569526672363\n",
            "Epoch 10 Training loss 2.9592569525684995\n",
            "Evaluation loss 3.81678158108577\n",
            "Epoch 11 iteration 0 loss 2.151359796524048\n",
            "Epoch 11 iteration 100 loss 2.970668077468872\n",
            "Epoch 11 iteration 200 loss 3.53497314453125\n",
            "Epoch 11 Training loss 2.8445405514370576\n",
            "Epoch 12 iteration 0 loss 2.0444366931915283\n",
            "Epoch 12 iteration 100 loss 2.853875160217285\n",
            "Epoch 12 iteration 200 loss 3.410889148712158\n",
            "Epoch 12 Training loss 2.7357957309704046\n",
            "Epoch 13 iteration 0 loss 1.9550198316574097\n",
            "Epoch 13 iteration 100 loss 2.776071310043335\n",
            "Epoch 13 iteration 200 loss 3.2924888134002686\n",
            "Epoch 13 Training loss 2.6349595360185423\n",
            "Epoch 14 iteration 0 loss 1.8444600105285645\n",
            "Epoch 14 iteration 100 loss 2.657776117324829\n",
            "Epoch 14 iteration 200 loss 3.186976194381714\n",
            "Epoch 14 Training loss 2.5394421565394762\n",
            "Epoch 15 iteration 0 loss 1.7708697319030762\n",
            "Epoch 15 iteration 100 loss 2.5296273231506348\n",
            "Epoch 15 iteration 200 loss 3.11570143699646\n",
            "Epoch 15 Training loss 2.449521600394243\n",
            "Evaluation loss 3.7337795226486548\n",
            "Epoch 16 iteration 0 loss 1.719935417175293\n",
            "Epoch 16 iteration 100 loss 2.4280879497528076\n",
            "Epoch 16 iteration 200 loss 3.0332415103912354\n",
            "Epoch 16 Training loss 2.365525131338882\n",
            "Epoch 17 iteration 0 loss 1.5855482816696167\n",
            "Epoch 17 iteration 100 loss 2.3588976860046387\n",
            "Epoch 17 iteration 200 loss 2.98225474357605\n",
            "Epoch 17 Training loss 2.2818982431982535\n",
            "Epoch 18 iteration 0 loss 1.5349936485290527\n",
            "Epoch 18 iteration 100 loss 2.2925095558166504\n",
            "Epoch 18 iteration 200 loss 2.878256320953369\n",
            "Epoch 18 Training loss 2.204249655141957\n",
            "Epoch 19 iteration 0 loss 1.5244817733764648\n",
            "Epoch 19 iteration 100 loss 2.176241397857666\n",
            "Epoch 19 iteration 200 loss 2.8098840713500977\n",
            "Epoch 19 Training loss 2.1306384096131255\n",
            "Epoch 20 iteration 0 loss 1.4187723398208618\n",
            "Epoch 20 iteration 100 loss 2.080695152282715\n",
            "Epoch 20 iteration 200 loss 2.7193922996520996\n",
            "Epoch 20 Training loss 2.0598650885186323\n",
            "Evaluation loss 3.7345552986519364\n",
            "Epoch 21 iteration 0 loss 1.317280888557434\n",
            "Epoch 21 iteration 100 loss 2.050288677215576\n",
            "Epoch 21 iteration 200 loss 2.625156879425049\n",
            "Epoch 21 Training loss 1.996216149977787\n",
            "Epoch 22 iteration 0 loss 1.266642689704895\n",
            "Epoch 22 iteration 100 loss 1.9768234491348267\n",
            "Epoch 22 iteration 200 loss 2.6190052032470703\n",
            "Epoch 22 Training loss 1.9332084553747653\n",
            "Epoch 23 iteration 0 loss 1.2166529893875122\n",
            "Epoch 23 iteration 100 loss 1.9323891401290894\n",
            "Epoch 23 iteration 200 loss 2.5362229347229004\n",
            "Epoch 23 Training loss 1.8726199170942714\n",
            "Epoch 24 iteration 0 loss 1.1830544471740723\n",
            "Epoch 24 iteration 100 loss 1.8851593732833862\n",
            "Epoch 24 iteration 200 loss 2.4663572311401367\n",
            "Epoch 24 Training loss 1.8154203668681277\n",
            "Epoch 25 iteration 0 loss 1.0970087051391602\n",
            "Epoch 25 iteration 100 loss 1.7759772539138794\n",
            "Epoch 25 iteration 200 loss 2.399458646774292\n",
            "Epoch 25 Training loss 1.764224098288763\n",
            "Evaluation loss 3.7631776162143304\n",
            "Epoch 26 iteration 0 loss 1.090941071510315\n",
            "Epoch 26 iteration 100 loss 1.699373483657837\n",
            "Epoch 26 iteration 200 loss 2.4038593769073486\n",
            "Epoch 26 Training loss 1.71458978984229\n",
            "Epoch 27 iteration 0 loss 1.0334160327911377\n",
            "Epoch 27 iteration 100 loss 1.6850180625915527\n",
            "Epoch 27 iteration 200 loss 2.3425638675689697\n",
            "Epoch 27 Training loss 1.6630518155273613\n",
            "Epoch 28 iteration 0 loss 1.0060240030288696\n",
            "Epoch 28 iteration 100 loss 1.6370056867599487\n",
            "Epoch 28 iteration 200 loss 2.265982151031494\n",
            "Epoch 28 Training loss 1.6193989603669199\n",
            "Epoch 29 iteration 0 loss 0.9438802599906921\n",
            "Epoch 29 iteration 100 loss 1.6318249702453613\n",
            "Epoch 29 iteration 200 loss 2.195892572402954\n",
            "Epoch 29 Training loss 1.5727306851618224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs--uAYiJ5Fu",
        "colab_type": "text"
      },
      "source": [
        "## 模型性能预览"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsZz1-14J5Fw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "98d3372e-b56a-4fe5-f4b0-4d6d14efa32e"
      },
      "source": [
        "\"\"\"定义翻译函数\"\"\"\n",
        "#翻译个句子看看结果咋样\n",
        "def translate_dev(i):\n",
        "    #随便取出句子\n",
        "    en_sent = \" \".join([inv_en_dict[w] for w in dev_en[i]])\n",
        "    print('待预测：',en_sent)\n",
        "    cn_sent = \" \".join([inv_cn_dict[w] for w in dev_cn[i]])\n",
        "    print('正确答案：',\"\".join(cn_sent))\n",
        "\n",
        "    mb_x = torch.from_numpy(np.array(dev_en[i]).reshape(1, -1)).long().to(device)\n",
        "    #把句子升维，并转换成tensor\n",
        "    \n",
        "    mb_x_len = torch.from_numpy(np.array([len(dev_en[i])])).long().to(device)\n",
        "    #取出句子长度，并转换成tensor\n",
        "    \n",
        "    bos = torch.Tensor([[cn_dict[\"BOS\"]]]).long().to(device)\n",
        "    #bos=tensor([[2]])\n",
        "\n",
        "    translation, attn = model.translate(mb_x, mb_x_len, bos)\n",
        "    #这里传入bos作为首个单词的输入\n",
        "    #translation=tensor([[ 8,  6, 11, 25, 22, 57, 10,  5,  6,  4]])\n",
        "    \n",
        "    translation = [inv_cn_dict[i] for i in translation.data.cpu().numpy().reshape(-1)]\n",
        "    trans = []\n",
        "    for word in translation:\n",
        "        if word != \"EOS\": # 把数值变成单词形式\n",
        "            trans.append(word) #\n",
        "        else:\n",
        "            break\n",
        "    print('机器翻译：',\"\".join(trans))\n",
        "\n",
        "for i in range(20,30):\n",
        "    translate_dev(i)\n",
        "    print()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "待预测： BOS anything else ? EOS\n",
            "正确答案： BOS 还有 别的 吗 ？ EOS\n",
            "机器翻译： 有什么人吃什么？\n",
            "\n",
            "待预测： BOS i 'm sleepy . EOS\n",
            "正确答案： BOS UNK 了 。 EOS\n",
            "机器翻译： 我是芬兰人。\n",
            "\n",
            "待预测： BOS i ate UNK . EOS\n",
            "正确答案： BOS 我 吃 了 UNK 。 EOS\n",
            "机器翻译： 我在吃面包。\n",
            "\n",
            "待预测： BOS i like sports . EOS\n",
            "正确答案： BOS 我 喜欢 运动 。 EOS\n",
            "机器翻译： 我喜欢足球。\n",
            "\n",
            "待预测： BOS she may come . EOS\n",
            "正确答案： BOS 她 可以 来 。 EOS\n",
            "机器翻译： 她现在可能来。\n",
            "\n",
            "待预测： BOS everybody will die . EOS\n",
            "正确答案： BOS 人 UNK UNK 。 EOS\n",
            "机器翻译： 每个人都在谈论了。\n",
            "\n",
            "待预测： BOS answer the question . EOS\n",
            "正确答案： BOS 回答 问题 。 EOS\n",
            "机器翻译： 问题是狗。\n",
            "\n",
            "待预测： BOS is that better ? EOS\n",
            "正确答案： BOS 那 更好 吗 ？ EOS\n",
            "机器翻译： 那是对吗？\n",
            "\n",
            "待预测： BOS i like you . EOS\n",
            "正确答案： BOS 我 喜欢 你 。 EOS\n",
            "机器翻译： 我喜欢你。\n",
            "\n",
            "待预测： BOS let him in . EOS\n",
            "正确答案： BOS 让 他 进来 。 EOS\n",
            "机器翻译： 让他把盐吧。\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLrgDuyHJ5GK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"模型保存\"\"\"\n",
        "\n",
        "torch.save(model.state_dict(), \"/content/luongAttention_seq2seq.pth\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1oTFONHJ5GV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "1d912790-763c-45cb-952e-842e51f6c52f"
      },
      "source": [
        "model"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embed): Embedding(5493, 100)\n",
              "    (rnn): GRU(100, 100, batch_first=True, bidirectional=True)\n",
              "    (dropout): Dropout(p=0.2)\n",
              "    (fc): Linear(in_features=200, out_features=100, bias=True)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embed): Embedding(8883, 100)\n",
              "    (attention): Attention(\n",
              "      (linear_in): Linear(in_features=200, out_features=100, bias=True)\n",
              "      (linear_out): Linear(in_features=300, out_features=100, bias=True)\n",
              "    )\n",
              "    (rnn): GRU(100, 100, batch_first=True)\n",
              "    (out): Linear(in_features=100, out_features=8883, bias=True)\n",
              "    (dropout): Dropout(p=0.2)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    }
  ]
}