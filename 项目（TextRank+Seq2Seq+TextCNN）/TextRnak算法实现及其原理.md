## TextRank算法实现及其原理

- #### PageRank算法（有向无权图模型）

  - ##### 简介：

    发明者谢尔盖布林认为，互联网中所有页面搜适用于一种特殊的层次机构：在一个页面中有越多的指向其他页面的链接，那么这个页面所获得的评分就越高，权值越大

  - ##### 随机游走模型（$random\;walk$）：j给定一个含有个n节点的有向图，算出 $W\,{ij}$表示节点 $i$ 跳转到节点 $j$ 的几率，最后得到一个n阶矩阵

    ​                                      $$\large M\;=\,[m_{ij}]_{n\times n}$$

    第 $i$ 行第 $$j$$ 列的元素 $$m_{ij}$$ 取值为 0 - 1 且 $\sum_{i=1}^{n}m_{ij}\;=\;1$

    那么随机游走在某个时刻t访问各个节点的概率分布就是马尔科夫链在时刻t的状态分布，可以用一个n维向量$R_t$来表示，那么在是个t+1访问各个节点的概率分布满足

    ​                                          $$R_{t+1}\;=\;M\,R_t$$

    假设t对任意自然数n  $\;t\;\in\;(1,2,...,n)\;$ 成立，那么极限向量$\;R_t\;$就表示为这个有向图的$\;PageRank\;$,$\;R\;$的各个分量称为各个节点的$\;PR(PageRank)\;$值，注意这个基本定义是在理想化的情况下，极限向量$\;R_t\;$一定存唯一值，换句话说t趋近于无穷的时候，$R\;$收敛于唯一的平稳分布

    遗憾的是，一般的有向图未必满足强连通且非周期性的条件，比如在互联网中，大部分的网页没有连接出去的超链接

  - ##### PageRank的一般定义

    为了解决很多的节点（网页）没有跳转到其他网页的超链接，那就给他一个平均分布，认为它跳转到其他n-1个网页的几率为$\;\frac{1}{n-1}\;$,又因为n趋于无穷，所以可以简化为$\;\frac{1}{n}\;$

    在这一定义下，可以定义一个新的马尔可夫链，容易证明这个马尔科夫链一定具有平稳分布（极限存在）而且平稳分布满足

    ​                                     $$\large R\,=\,dMR\,+(1-d)$$

    其中$d\isin(0,1)\;$是系数，称为阻尼因子，$R\;$是$\;n\;$维向量表示有向图的一般PageRank，1是左右分量为1的n维向量，n是n维向量

    第二项（1-d）为平滑项，使得所有的节点符合$\;PR(vi)\,>0, i=1,2,...,n$

    ~~~python
  # 初始化转换矩阵M
    M = np.array([[0,   1, 1],
                  [0.5, 0, 0],
                  [0.5, 0, 0]])
    # 初始化R值
    R = np.array([[1,1,1]]).reshape(-1,1)
    
    # 取基尼系数为0.85，开始迭代更新R值
    for i in range(100):
        R = 0.15 + 0.85*np.matmul(M,R)
        print(R)
    ~~~
  
    ~~~shell
  [[1.45945963]
     [0.77027018]
     [0.77027018]]
    ~~~
  
    显而易见，ABC页面中A的PR值更大，表示A更重要

- #### TextRank算法

  - 简介：

    本质上，它在专门为特定的NLP任务设计的图表上运行PangRank，对于关键词提取，他使用一些文本单元集作为定点来构件图。边是基于文本单元顶点之间的语义或词汇相似度的度量。与PageRank不同的是，边缘一般是无向的，可加权以反映相似程度

  - ##### 原理：

    1. 文本预处理

       将一篇文本产分为句子，在每个句子中去除停用词（可选），并保留指定词性的单词（可选）。由此可以得到句子的集合和单词的集合

    2. #### TextRank提取关键词模型（无向无权图模型）

       将每个**单词**作为PageRank的一个节点，假设文中有n个词，设定窗口大小为k，假设一个句子依次由下面的单词构成

       ​                                    $$w_1,w_2,w_3,...w_n$$

       那么$$\;[w_1,w_2,..,w_k],[w_2,w_3,...,w_{k+1}]\;$$等都是一个窗口，在一个窗口中的任意两个单词$w$对应的节点之间存在一个**无向无权**的边，也就是说

       1. 词与词之间的关联没有权重
       2. 每个词不是与文章中的所有其他词都有“链接”

       所以这么一来，TextRank提取关键词的时候，与PageRank的算法是一样的

       ​                                            $$\large W_{word}=(1-d)+d*M*W_{word}$$

       迭代计算提取$W_{word}$ 就可以得到每个词的重要程度，作倒序，取前面的几个作为关键词

    3. #### TextRank提取关键句模型（无向有权图模型）

       将每个句子作为类似PageRank的一个节点，若两个句子之间有相似性，认为对应的两个节点之间由一个**无向有权**的边，权值是相似度，论文中使用下面的方式计算相似度

       

       ​                       $$\large W_{ij}=Similarity(S_i,S_j)=\frac{S_i,S_j中相同的词的数量}{log(S_i的词数)+log(S_j中的词数)}$$

       

       ​      $$\large W_{sentence}(V_j)=(1-d)+d*\sum_{V_{j}\isin In(V_i)} \frac{W_{ij}}{\sum_{V_k\isin Out(V_j) }W_{jk}}*W_{sentence}(V_j)$$

       

       迭代计算，将得到的结果的分进行倒序排序，抽取重要程度最高的几个句子作为关键句

  - #### 几个问题

    - ##### 窗口大小$Window$参数的选择？

      论文显示$Window=2$时候效果最好，准确率最高

    - ##### TextRank中关键词抽取的转移矩阵$M$中$W_{ij}$，既然是无向的该怎么初始化?
  
      论文显示只要在窗口中共现过的单词就认为有关系，就把他们的边$Edge$设为1，也就是说$W_{ij}$设为1，注意最后的$Rank$向量与$W_{ij}$的初始值取值大小无关，与是否为0有关
  
    - ##### 无向和有向对模型的影响？
  
      有向的图都可以改造成无向的图，有趣的是，论文中实验表明，对于稀松连接的图模型（$Win$比较小），无向图反而随着迭代运算收敛的更快
  
    - ##### 迭代运算词数是否越多越好？
  
      理论上是的，但是论文中大多数的图模型经过20次迭代运算后都已经达到收敛
  
  - #### 范例
  
    使用前必须要安装包
  
    `pip install TextRank4ZH`
  
    ~~~python
    import codecs
    from textrank4zh import TextRank4Keyword, TextRank4Sentence
    
    text = codecs.open('../test/doc/01.txt', 'r', 'utf-8').read()
    tr4w = TextRank4Keyword()
    
    tr4w.analyze(text=text, lower=True, window=2)  # py2中text必须是utf8编码的str或者unicode对象，py3中必须是utf8编码的bytes或者str对象
  
    print( '关键词：' )
  for item in tr4w.get_keywords(20, word_min_len=1):
        print(item.word, item.weight)
    
    print()
    print( '关键短语：' )
    for phrase in tr4w.get_keyphrases(keywords_num=20, min_occur_num= 2):
        print(phrase)
    
    tr4s = TextRank4Sentence()
    tr4s.analyze(text=text, lower=True, source = 'all_filters')
    
    print()
    print( '摘要：' )
    for item in tr4s.get_key_sentences(num=3):
        print(item.index, item.weight, item.sentence)  # index是语句在文本中位置，weight是权重
    ~~~
  
    输出的结果
  
    ~~~shell
  关键词：
    媒体 0.02155864734852778
    高圆圆 0.020220281898126486
    微 0.01671909730824073
    ...
    戴 0.008915271161035208
    酒店 0.00883521621207796
    外套 0.008822082954131174
    
    关键短语：
    微博
    
    摘要：
    摘要：
    0 0.0709719557171 中新网北京12月1日电(记者 张曦) 30日晚，高圆圆和赵又廷在京举行答谢宴，诸多明星现身捧场，其中包括张杰(微博)、谢娜(微博)夫妇、何炅(微博)、蔡康永(微博)、徐克、张凯丽、黄轩(微博)等
    6 0.0541037236415 高圆圆身穿粉色外套，看到大批记者在场露出娇羞神色，赵又廷则戴着鸭舌帽，十分淡定，两人快步走进电梯，未接受媒体采访
    27 0.0490428312984 记者了解到，出席高圆圆、赵又廷答谢宴的宾客近百人，其中不少都是女方的高中同学
    ~~~
  
    